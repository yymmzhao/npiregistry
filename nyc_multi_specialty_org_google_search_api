After get a list of organizaitons with target taxonomy, and for those who have multi-spetialty in their evaluation services column, connect google search API to find the webistes for each organization
Steps before running the sciprt: 
Step 1: Get Your API Key:
- Go to: https://console.cloud.google.com/
- Create/Select Project
- Enable API: Search for "Custom Search API" and enable it
- Create Credentials: Go to "Credentials" ‚Üí "Create Credentials" ‚Üí "API Key"
- Copy your API key
Step 2: Get Your Search Engine ID:
- Go to: https://cse.google.com/
- Click "Add" to create new search engine
- Sites to search: Enter * (to search the entire web)
- Create and copy your "Search engine ID"

The web search strategy in the current script:
Logic - Domain-First Priority
PHASE 1: DOMAIN EXTRACTION & PRIORITY CHECK
StepFunctionActionDecision Point1.1Extract domain/pathParse URL componentsGet clean domain1.2check_domain_name_variations()Check domain for name matchesDECISION POINT1.3aIF domain matchesSkip to directory filteringGo to Phase 2A1.3bIF domain NO matchCheck other URL partsGo to Phase 2B
PHASE 2A: DOMAIN MATCH FOUND (Priority Path)
StepFunctionActionPoints/Result2A.1is_directory_or_listing()Filter directories0 points if directory2A.2fetch_page_content()Get webpage contentRequired for scoring2A.3Score domain matchDomain scoring only25-60 points2A.4Add address bonusAddress matching+0-20 points2A.5Add services bonusServices matching+0-15 points2A.6Check thresholdTotal ‚â• 30?ACCEPT immediately
PHASE 2B: NO DOMAIN MATCH (Secondary Path)
StepFunctionActionPoints/Result2B.1check_path_name_variations()Check URL path for matchesContinue if found2B.2check_subdomain_variations()Check subdomain for matchesContinue if found2B.3is_directory_or_listing()Filter directories0 points if directory2B.4Score URL componentsPath/subdomain scoring10-25 points2B.5Add address bonusAddress matching+0-20 points2B.6Add services bonusServices matching+0-15 points2B.7Check thresholdTotal ‚â• 30?Accept or reject
DOMAIN MATCHING HIERARCHY
PriorityCheckPointsExampleAction1stMemorial special case60memorial ‚Üí mskcc.orgACCEPT (skip other checks)2ndMulti-length abbreviations50es in eslonestarACCEPT (skip other checks)3rdPrimary keywords (2+)45easter + sealsACCEPT (skip other checks)4thSingle primary keyword35easter OR sealsACCEPT (skip other checks)5thHigh similarity (>70%)30String matchACCEPT (skip other checks)
EFFICIENCY GAINS
ScenarioOld LogicNew LogicTime SavedPerfect domain matchCheck domain ‚Üí path ‚Üí subdomain ‚Üí score allCheck domain ‚Üí STOP ‚Üí score~60% fasterGood domain matchCheck all URL partsDomain match ‚Üí STOP~50% fasterNo domain matchCheck all parts anywayCheck other parts onlySame speed
DECISION TREE
URL Input
    ‚Üì
Domain has name variations?
    ‚ÜìYES ‚Üí Is directory site? 
        ‚ÜìNO ‚Üí Score domain + address + services ‚Üí Accept if ‚â•30
    ‚ÜìNO ‚Üí Path/subdomain has variations?
        ‚ÜìYES ‚Üí Is directory site?
            ‚ÜìNO ‚Üí Score path + address + services ‚Üí Accept if ‚â•30
        ‚ÜìNO ‚Üí Reject (no name match anywhere)

The sciprt starts below this line
-----
import requests
import pandas as pd
import time
import re
from urllib.parse import urlparse
from difflib import SequenceMatcher
import os
from datetime import datetime

class WebsiteFinderV33:
    def __init__(self, api_key, search_engine_id):
        self.api_key = api_key
        self.search_engine_id = search_engine_id
        self.base_url = "https://www.googleapis.com/customsearch/v1"
        self.requests_made = 0
    
    def clean_organization_name(self, org_name):
        """Clean organization name for search - remove legal suffixes and punctuation"""
        if pd.isna(org_name):
            return ""
        
        cleaned = str(org_name).strip()
        
        # Remove legal suffixes
        suffixes = ['PLLC', 'P.L.L.C.', 'PC', 'P.C.', 'LLC', 'L.L.C.', 
                   'INC', 'CORP', 'LTD', 'LLP', 'L.L.P.']
        
        for suffix in suffixes:
            # Remove with word boundaries
            cleaned = re.sub(rf'\b{suffix}\b', '', cleaned, flags=re.IGNORECASE)
        
        # Remove trailing punctuation
        cleaned = cleaned.rstrip(',.;:')
        
        # Clean up multiple spaces
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        return cleaned
    
    def extract_keywords_from_org_name(self, org_name):
        """ENHANCED: Extract meaningful keywords with better punctuation handling"""
        cleaned = self.clean_organization_name(org_name).lower()
        
        # Remove ALL punctuation and clean
        cleaned = re.sub(r'[^\w\s]', ' ', cleaned)  # Remove punctuation
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()  # Normalize spaces
        
        # Remove common words but keep important ones
        stop_words = ['the', 'of', 'for', 'and', 'in', 'at', 'to', 'a', 'an']
        words = [w for w in cleaned.split() if w not in stop_words and len(w) > 2]
        
        return words
    
    def get_name_variations(self, keywords):
        """Generate focused name variations - multi-length abbreviations + keywords"""
        variations = []
        
        if not keywords:
            return variations
        
        # Multi-length abbreviation patterns (from original analysis)
        if len(keywords) >= 2:
            variations.append(''.join([w[0] for w in keywords[:2]]))  # es
        if len(keywords) >= 3:
            variations.append(''.join([w[0] for w in keywords[:3]]))  # esc
        if len(keywords) >= 4:
            variations.append(''.join([w[0] for w in keywords[:4]]))  # esct
        
        # Primary keywords (first 2 words)
        variations.extend(keywords[:2])
        
        # Secondary keywords (words 3-4)
        if len(keywords) > 2:
            variations.extend(keywords[2:4])
        
        # Remove duplicates and clean
        clean_variations = []
        for var in set(variations):
            if var and len(var) >= 2:
                clean_variations.append(var.lower())
        
        return clean_variations
    
    def extract_service_keywords(self, evaluation_services):
        """Extract service keywords from evaluation services field"""
        if not evaluation_services or pd.isna(evaluation_services):
            return []
        
        service_map = {
            'audiolog': ['audiolog', 'hearing', 'auditory'],
            'speech': ['speech', 'language', 'communication'],
            'physical therapy': ['physical therapy', 'pt', 'physiotherapy'],
            'occupational therapy': ['occupational therapy', 'ot', 'occupational'],
            'mental health': ['mental health', 'counseling', 'therapy', 'psychiatric', 'psychology'],
            'developmental': ['developmental', 'autism', 'behavioral']
        }
        
        keywords = []
        services_lower = evaluation_services.lower()
        
        for key, terms in service_map.items():
            if any(term in services_lower for term in terms):
                keywords.extend(terms)
        
        return list(set(keywords))
    
    def check_domain_name_variations(self, url, org_name):
        """PRIORITY CHECK: Check if domain contains name variations (Phase 1)"""
        try:
            domain = urlparse(url).netloc.lower()
            domain_clean = re.sub(r'^www\.', '', domain)
            domain_clean = re.sub(r'\.(com|org|net|edu|gov)$', '', domain_clean)
            
            org_keywords = self.extract_keywords_from_org_name(org_name)
            org_name_lower = org_name.lower()
            
            if not org_keywords:
                return False, 0
            
            # SPECIAL CASE: Memorial Sloan Kettering (check FIRST)
            if ('memorial' in org_name_lower or 'sloan kettering' in org_name_lower):
                if domain_clean in ['mskcc', 'msk']:
                    return True, 60  # Maximum score
            
            # Get name variations
            variations = self.get_name_variations(org_keywords)
            
            # Check multi-length abbreviations (highest priority)
            abbrevs = []
            if len(org_keywords) >= 2:
                abbrevs.append(''.join([w[0] for w in org_keywords[:2]]))
            if len(org_keywords) >= 3:
                abbrevs.append(''.join([w[0] for w in org_keywords[:3]]))
            if len(org_keywords) >= 4:
                abbrevs.append(''.join([w[0] for w in org_keywords[:4]]))
            
            for abbrev in abbrevs:
                if len(abbrev) >= 2 and abbrev in domain_clean:
                    return True, 50  # High score for abbreviation match
            
            # Check primary keywords (2+ matches = definitive) - FIXED: >=3 instead of >=4
            primary_keywords = org_keywords[:2]
            primary_matches = sum(1 for word in primary_keywords if len(word) >= 3 and word.lower() in domain_clean)
            
            if primary_matches >= 2:
                return True, 45  # High score for multiple keywords
            elif primary_matches == 1:
                return True, 35  # Good score for single primary keyword
            
            # Check partial keywords (first 3-4 chars) - FIXES lexington -> lex
            for keyword in org_keywords[:3]:  # Check first 3 keywords
                if len(keyword) >= 4:
                    # Check first 3 characters
                    partial3 = keyword[:3].lower()
                    if len(partial3) >= 3 and partial3 in domain_clean:
                        return True, 30  # Partial match score
                    
                    # Check first 4 characters for longer words
                    if len(keyword) >= 6:
                        partial4 = keyword[:4].lower()
                        if partial4 in domain_clean:
                            return True, 35  # Longer partial match
            
            # Check secondary keywords - FIXED: >=3 instead of >=4
            secondary_keywords = org_keywords[2:4] if len(org_keywords) > 2 else []
            secondary_matches = sum(1 for word in secondary_keywords if len(word) >= 3 and word.lower() in domain_clean)
            
            if secondary_matches >= 1 and primary_matches >= 1:
                return True, 40  # Combined primary + secondary
            
            # Check high similarity
            org_name_clean = ' '.join(org_keywords[:3])
            similarity = self.similarity_score(org_name_clean, domain_clean)
            
            if similarity > 0.7:
                return True, 30  # Similarity match
            
            return False, 0  # No domain match found
            
        except:
            return False, 0
    
    def check_path_name_variations(self, url, org_name):
        """Check URL path for name variations (Phase 2B)"""
        try:
            path = urlparse(url).path.lower()
            path_clean = path.replace('/', ' ').replace('-', ' ').replace('_', ' ')
            
            org_keywords = self.extract_keywords_from_org_name(org_name)
            
            if not org_keywords:
                return False, 0
            
            variations = self.get_name_variations(org_keywords)
            
            # Check for variations in path
            for variation in variations:
                if len(variation) >= 3 and variation in path_clean:
                    if len(variation) >= 6:  # Long variations
                        return True, 30
                    elif len(variation) >= 4:  # Medium variations
                        return True, 25
                    else:  # Short variations
                        return True, 20
            
            return False, 0
            
        except:
            return False, 0
    
    def check_subdomain_name_variations(self, url, org_name):
        """Check subdomain for name variations (Phase 2B)"""
        try:
            domain_parts = urlparse(url).netloc.lower().split('.')
            
            if len(domain_parts) <= 2:  # No subdomain
                return False, 0
            
            subdomain = domain_parts[0]  # First part is subdomain
            if subdomain == 'www':
                return False, 0  # Skip www
            
            org_keywords = self.extract_keywords_from_org_name(org_name)
            
            if not org_keywords:
                return False, 0
            
            variations = self.get_name_variations(org_keywords)
            
            # Check for variations in subdomain
            for variation in variations:
                if len(variation) >= 2 and variation in subdomain:
                    if len(variation) >= 4:
                        return True, 35  # High score for subdomain match
                    else:
                        return True, 30
            
            return False, 0
            
        except:
            return False, 0
    
    def is_directory_or_listing(self, url, org_name):
        """Directory filtering with organization domain protection"""
        try:
            domain = urlparse(url).netloc.lower()
            path = urlparse(url).path.lower()
            
            # Step 1: ALWAYS filter known directory domains
            directory_domains = [
                'healthyhearing.com', 'healthgrades.com', 'vitals.com', 'zocdoc.com',
                'yelp.com', 'yellowpages.com', 'superpages.com', 'manta.com',
                'doximity.com', 'healthcare4ppl.com', 'npidb.org', 'npino.com',
                'mapquest.com', 'usnews.com', 'wellness.com', 'webmd.com',
                'mentalhealthcenters.net', 'mentalhealthfacilities.net',
                'esad.org', 'psychiatry.org', 'psychologytoday.com',
                'linkedin.com', 'facebook.com', 'twitter.com', 'instagram.com',
                'indeed.com', 'glassdoor.com', 'ratemds.com', 'findatopdoc.com',
                'npiregistry.cms.hhs.gov', 'cms.gov', 'medicare.gov',
                'bbb.org', 'corporationwiki.com', 'bizapedia.com',
                'opengovus.com', 'causeiq.com', 'guidestar.org', 'crunchbase.com'
            ]
            
            for dir_domain in directory_domains:
                if dir_domain in domain:
                    return True  # Always filter directory domains
            
            # Step 2: Check if it's organization's domain (PROTECTION)
            has_domain_match, _ = self.check_domain_name_variations(url, org_name)
            if has_domain_match:
                return False  # NEVER filter organization's own domain
            
            # Step 3: Apply path-based filters to non-org domains
            directory_patterns = [
                '/directory/', '/find-', '/search/', '/provider/', '/providers/',
                '/doctor/', '/doctors/', '/physician/', '/listing/', '/listings/',
                '/business/', '/resources/', '/news/', '/article/', '/blog/', 
                '/press/', '/media/', '/event/', '/events/'
            ]
            
            for pattern in directory_patterns:
                if pattern in path:
                    return True  # Filter non-org domains with these paths
            
            return False  # Allow everything else
            
        except:
            return False
    
    def count_exact_keyword_matches(self, url, org_name):
        """Count exact keyword matches for tie-breaking"""
        try:
            domain = urlparse(url).netloc.lower()
            domain_clean = re.sub(r'^www\.', '', domain)
            domain_clean = re.sub(r'\.(com|org|net|edu|gov)$', '', domain_clean)
            
            org_keywords = self.extract_keywords_from_org_name(org_name)
            
            exact_matches = 0
            partial_matches = 0
            
            for keyword in org_keywords:
                if len(keyword) >= 4:
                    if keyword in domain_clean:
                        exact_matches += 1
                    elif len(keyword) >= 3 and any(keyword[:3] in domain_clean or keyword[-3:] in domain_clean for i in range(len(keyword)-2)):
                        partial_matches += 1
            
            return exact_matches, partial_matches
        except:
            return 0, 0
    
    def fetch_page_content(self, url):
        """Fetch content from a webpage"""
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            response = requests.get(url, headers=headers, timeout=10)
            return response.text
        except:
            return ""
    
    def normalize_address(self, address):
        """Normalize address format for matching"""
        if not address or pd.isna(address):
            return ""
        
        address_str = str(address).lower().strip()
        
        # Standard abbreviations
        replacements = {
            'street': 'st', 'avenue': 'ave', 'boulevard': 'blvd',
            'drive': 'dr', 'road': 'rd', 'lane': 'ln',
            'suite': 'ste', 'apartment': 'apt', 'floor': 'fl',
            'north': 'n', 'south': 's', 'east': 'e', 'west': 'w'
        }
        
        for full, abbrev in replacements.items():
            address_str = re.sub(rf'\b{full}\b', abbrev, address_str)
        
        # Clean punctuation and normalize spaces
        address_str = re.sub(r'[^\w\s]', ' ', address_str)
        address_str = re.sub(r'\s+', ' ', address_str).strip()
        
        return address_str
    
    def similarity_score(self, str1, str2):
        """Calculate similarity score between two strings"""
        if not str1 or not str2:
            return 0
        return SequenceMatcher(None, str1.lower(), str2.lower()).ratio()
    
    def extract_all_addresses(self, text):
        """Extract all potential addresses from webpage content"""
        if not text:
            return []
        
        # Pattern for street addresses
        address_pattern = r'\d+[\s-]*\d*\s+[\w\s]+?\s+(?:street|st|avenue|ave|road|rd|drive|dr|lane|ln|boulevard|blvd|way|court|ct|circle|cir|place|pl)'
        addresses = re.findall(address_pattern, text, re.IGNORECASE)
        return list(set(addresses))[:10]  # Limit to 10 unique addresses
    
    def check_address_match(self, page_content, target_address):
        """Check if target address appears on webpage"""
        if not page_content or not target_address:
            return 0
        
        target_normalized = self.normalize_address(target_address)
        if len(target_normalized) < 10:  # Skip very short addresses
            return 0
        
        found_addresses = self.extract_all_addresses(page_content)
        
        for address in found_addresses:
            address_normalized = self.normalize_address(address)
            similarity = self.similarity_score(target_normalized, address_normalized)
            
            if similarity > 0.6:
                return 20  # Address match bonus
        
        return 0
    
    def check_services_match(self, page_content, service_keywords):
        """Check if service keywords appear on webpage"""
        if not page_content or not service_keywords:
            return 0
        
        content_lower = page_content.lower()
        matches = sum(1 for kw in service_keywords if kw in content_lower)
        
        if matches >= 1:
            return 15  # Service match bonus
        
        return 0
    
    def search_organization_website(self, org_name):
        """Search for organization website using Google Custom Search API"""
        try:
            clean_name = self.clean_organization_name(org_name)
            search_query = f'"{clean_name}"'
            
            params = {
                'key': self.api_key,
                'cx': self.search_engine_id,
                'q': search_query,
                'num': 10
            }
            
            response = requests.get(self.base_url, params=params, timeout=10)
            self.requests_made += 1
            
            if response.status_code != 200:
                return []
            
            results = response.json()
            
            if 'items' not in results:
                return []
            
            candidates = []
            for item in results['items']:
                url = item.get('link', '')
                if url and not url.lower().endswith(('.pdf', '.doc', '.docx')):
                    candidates.append(url)
            
            # Get second page of results (11-20)
            params['start'] = 11
            response2 = requests.get(self.base_url, params=params, timeout=10)
            self.requests_made += 1
            
            if response2.status_code == 200:
                results2 = response2.json()
                if 'items' in results2:
                    for item in results2['items']:
                        url = item.get('link', '')
                        if url and not url.lower().endswith(('.pdf', '.doc', '.docx')):
                            candidates.append(url)
            
            return candidates
            
        except Exception as e:
            return []
    
    def score_candidate(self, url, org_name, address, service_keywords):
        """FIXED: Domain-prioritized scoring with proper directory filtering"""
        
        # PHASE 0: Always check directory filtering FIRST (before any matching)
        if self.is_directory_or_listing(url, org_name):
            return 0, 0, False, ["Directory/listing site (excluded)"]
        
        # PHASE 1: Check domain for name variations (PRIORITY)
        has_domain_match, domain_score = self.check_domain_name_variations(url, org_name)
        
        if has_domain_match:
            # PHASE 2A: Domain match found - Priority path (already passed directory filter)
            
            # Fetch page content directly
            page_content = self.fetch_page_content(url)
            if not page_content:
                return 0, 0, has_domain_match, ["Could not fetch content"]
            
            total_score = domain_score
            details = [f"Domain: +{domain_score}"]
            
            # Add address bonus
            address_score = self.check_address_match(page_content, address)
            if address_score > 0:
                total_score += address_score
                details.append(f"Address: +{address_score}")
            
            # Add services bonus
            service_score = self.check_services_match(page_content, service_keywords)
            if service_score > 0:
                total_score += service_score
                details.append(f"Services: +{service_score}")
            
            # Domain strength for tie-breaking
            exact_matches, partial_matches = self.count_exact_keyword_matches(url, org_name)
            domain_strength = exact_matches * 2 + partial_matches
            
            return total_score, domain_strength, has_domain_match, details
        
        else:
            # PHASE 2B: No domain match - Check other URL parts (already passed directory filter)
            
            # Check path for variations
            has_path_match, path_score = self.check_path_name_variations(url, org_name)
            
            # Check subdomain for variations  
            has_subdomain_match, subdomain_score = self.check_subdomain_name_variations(url, org_name)
            
            # If no variations found anywhere, reject
            if not has_path_match and not has_subdomain_match:
                return 0, 0, False, ["No name variations found in URL"]
            
            # Fetch page content (directory filtering already done)
            page_content = self.fetch_page_content(url)
            if not page_content:
                return 0, 0, False, ["Could not fetch content"]
            
            # Calculate total score from URL parts
            total_score = max(path_score, subdomain_score)
            details = []
            
            if has_path_match:
                details.append(f"Path: +{path_score}")
            if has_subdomain_match:
                details.append(f"Subdomain: +{subdomain_score}")
            
            # Add address bonus
            address_score = self.check_address_match(page_content, address)
            if address_score > 0:
                total_score += address_score
                details.append(f"Address: +{address_score}")
            
            # Add services bonus
            service_score = self.check_services_match(page_content, service_keywords)
            if service_score > 0:
                total_score += service_score
                details.append(f"Services: +{service_score}")
            
            # Lower domain strength for non-domain matches
            domain_strength = 1 if has_subdomain_match else 0
            
            return total_score, domain_strength, False, details
    
    def find_best_website(self, org_name, address, evaluation_services):
        """Find best matching website with improved scoring"""
        print(f"\nüè• Processing: {org_name}")
        print(f"   üìç Address: {address}")
        print(f"   üîç Search query: \"{self.clean_organization_name(org_name)}\"")
        
        # Extract keywords and variations
        org_keywords = self.extract_keywords_from_org_name(org_name)
        variations = self.get_name_variations(org_keywords)
        print(f"   üîë Keywords: {org_keywords}")
        print(f"   üè∑Ô∏è  Variations: {variations}")
        print(f"   üìä Total variations: {len(variations)}")
        
        # Extract service keywords
        service_keywords = self.extract_service_keywords(evaluation_services)
        
        # Search for candidates
        candidates = self.search_organization_website(org_name)
        
        if not candidates:
            print(f"   ‚ùå No search results found")
            return None
        
        print(f"   üìã Found {len(candidates)} candidates")
        
        # Score all candidates
        scored_candidates = []
        
        for url in candidates:
            score, domain_strength, is_org_domain, details = self.score_candidate(
                url, org_name, address, service_keywords
            )
            
            domain = urlparse(url).netloc.lower()
            
            if score > 0:
                scored_candidates.append({
                    'url': url,
                    'score': score,
                    'domain_strength': domain_strength,
                    'is_org_domain': is_org_domain,
                    'details': details
                })
                print(f"   ‚úì {url[:60]}... ‚Üí Score: {score} | Org: {is_org_domain} | {' | '.join(details[:2])}")
            else:
                print(f"   ‚úó {url[:60]}... ‚Üí {details[0] if details else 'Score: 0'}")
            
            time.sleep(0.3)  # Rate limiting
        
        if not scored_candidates:
            print(f"   ‚ùå NO MATCH (no candidates scored above 0)")
            return None
        
        # Sort by: (1) Score DESC, (2) Is org domain (True first), (3) Domain strength DESC
        scored_candidates.sort(
            key=lambda x: (x['score'], x['is_org_domain'], x['domain_strength']),
            reverse=True
        )
        
        # Accept best candidate if score >= 30
        THRESHOLD = 30
        best = scored_candidates[0]
        
        if best['score'] >= THRESHOLD:
            print(f"   ‚úÖ ACCEPTED: {best['url']}")
            print(f"   üìä Final score: {best['score']} | Org domain: {best['is_org_domain']} | Domain strength: {best['domain_strength']}")
            print(f"   üìù Details: {' | '.join(best['details'])}")
            return best['url']
        else:
            print(f"   ‚ùå NO MATCH (best score: {best['score']}, threshold: {THRESHOLD})")
            return None

def process_organizations_with_resume(input_file, output_file, api_key, search_engine_id, max_api_calls=1450):
    """CLEAN START: Process organizations with proper column positioning and no duplicates"""
    print("=" * 80)
    print("WEBSITE FINDER - VERSION 3.3 (CLEAN START)")
    print("=" * 80)
    print(f"Max API calls per run: {max_api_calls}")
    print("=" * 80)
    
    # Check if output file exists (resume mode)
    if os.path.exists(output_file):
        print(f"\nüîÑ RESUME MODE: Found existing {output_file}")
        df = pd.read_csv(output_file)
        print(f"   Loaded {len(df)} organizations")
        
        processed_count = df['Processed'].sum() if 'Processed' in df.columns else 0
        remaining_count = len(df) - processed_count
        websites_found = df['Website'].notna().sum() if 'Website' in df.columns else 0
        print(f"   ‚úÖ Already processed: {processed_count}")
        print(f"   üìä Websites found: {websites_found}")
        print(f"   ‚è≥ Remaining: {remaining_count}")
        
        backup_file = output_file.replace('.csv', f'_backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv')
        df.to_csv(backup_file, index=False)
        print(f"   üíæ Backup created: {backup_file}")
    else:
        print(f"\nüìÇ Starting fresh: Loading {input_file}")
        df = pd.read_csv(input_file)
        print(f"   Loaded {len(df)} organizations")
        
        # Find position of "Evaluation Services" column
        eval_services_col = 'Evaluation Services'
        if eval_services_col in df.columns:
            eval_pos = df.columns.get_loc(eval_services_col)
            
            # Insert Website column right after Evaluation Services
            df.insert(eval_pos + 1, 'Website', '')
            print(f"   ‚úÖ Added 'Website' column after 'Evaluation Services'")
            
            # Insert Processed column right after Website
            df.insert(eval_pos + 2, 'Processed', False)
            print(f"   ‚úÖ Added 'Processed' column after 'Website'")
        else:
            # Fallback if column not found
            df['Website'] = ''
            df['Processed'] = False
            print(f"   ‚úÖ Added 'Website' and 'Processed' columns at end")
    
    # Initialize finder
    finder = WebsiteFinderV33(api_key, search_engine_id)
    
    print(f"\nüöÄ Starting website search with V3.3 clean start...")
    print(f"Will stop at {max_api_calls} API calls\n")
    
    organizations_checked = 0
    organizations_skipped = 0
    websites_found = 0
    
    for idx, row in df.iterrows():
        # IMPROVED RESUME: Skip if already processed 
        if 'Processed' in df.columns and row['Processed'] == True:
            organizations_skipped += 1
            continue
        
        # Get organization name using original NPI column name (NO DUPLICATES)
        org_name = str(row.get('Provider Organization Name (Legal Business Name)', ''))
        
        # TEMPORARY FIX: Skip problematic organization that hangs
        if 'SHIFT PHYSICAL THERAPY' in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if 'NEW LIFE MEDICAL PC' in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue
            
        if 'NEW PATHWAYS COUNSELING CENTER' in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if 'COGNITIVE BEHAVIORAL PSYCHOLOGY OF NY PC' in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if 'PSYCHOLOGICAL EVALUATION AND TESTING SERVICES' in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if 'ROSE HILL PSYCHOLOGICAL SERVICES, PC' in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if "CHILDREN'S THERAPY CORNER" in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if "THERAPIST NETWORK, INC" in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if "AZIMUTH MEDICAL PC" in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if "SUPPORTIVE PSYCHOLOGICAL SERVICES PC" in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if "NEW YORK ANXIETY TREATMENT CENTER" in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if "KARAN JOHAR, MD, PLLC" in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if "THRIVE PSYCHOLOGICAL SERVICES PLLC" in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if "INTEGRATIVE PSYCHOLOGICAL SERVICES, PLLC" in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if "SPEAK, LEARN, AND PLAY, LLC" in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue
    
        if "CITYLINK PSYCHOLOGICAL SERVICES, PLLC" in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if "MAX MALITZKY LLC" in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if "GROUNDWORK THERAPY PSYCHOLOGICAL SERVICES, PLLC" in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if "ALEXANDRA PETROU" in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if "NEW PERSPECTIVE PSYCHOLOGICAL SERVICES" in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if "ANGELA YEUNG PSY.D. PLLC" in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if "ELITE MOVEMENT INITIATIVE LLC" in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if "WELLNESS ROAD PSYCHOLOGY, PLLC" in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue

        if "BROOKLYN INTEGRATIVE PSYCHOLOGICAL SERVICES, P.L.L.C." in org_name.upper():
            print(f"‚è≠Ô∏è  SKIPPING (problematic): {org_name}")
            df.at[idx, 'Processed'] = True  # Mark as processed so it doesn't retry
            organizations_skipped += 1
            continue
        
        # Check API limit
        if finder.requests_made >= max_api_calls:
            print(f"\n‚ö†Ô∏è  REACHED API LIMIT ({max_api_calls} calls)")
            print(f"Stopping to stay within limit")
            print(f"Progress saved. Run again to continue.")
            break
        
        organizations_checked += 1
        
        # Get organization details using original NPI column names (NO DUPLICATES)
        address = str(row.get('Provider First Line Business Practice Location Address', ''))
        evaluation_services = str(row.get('Evaluation Services', ''))
        
        # Find website
        website = finder.find_best_website(org_name, address, evaluation_services)
        
        # Update dataframe - ALWAYS mark as processed
        df.at[idx, 'Processed'] = True
        if website:
            df.at[idx, 'Website'] = website
            websites_found += 1
        
        # Auto-save every 10 organizations
        if organizations_checked % 10 == 0:
            df.to_csv(output_file, index=False)
            print(f"\nüíæ Progress saved!")
            print(f"üìä Progress: {organizations_checked} checked | {websites_found} found | API calls: {finder.requests_made}/{max_api_calls}\n")
        
        # Rate limiting
        time.sleep(1)
    
    # Final save
    df.to_csv(output_file, index=False)
    print(f"\nüíæ Final save complete!")
    print(f"\nüìä FINAL STATISTICS:")
    print(f"   Organizations checked: {organizations_checked}")
    print(f"   Organizations skipped: {organizations_skipped}")
    print(f"   Websites found: {websites_found}")
    print(f"   Success rate: {(websites_found/organizations_checked*100 if organizations_checked > 0 else 0):.1f}%")
    print(f"   API calls made: {finder.requests_made}/{max_api_calls}")
    print(f"   Output file: {output_file}")
    
    return df

# Main execution
if __name__ == "__main__":
    print("Website Finder - VERSION 3.3 (CLEAN START)")
    print("=" * 45)
    print("\nV3.3 Clean Start Features:")
    print("‚úÖ CLEAN: No duplicate columns, uses original NPI column names")
    print("‚úÖ POSITIONING: Website column after 'Evaluation Services'")
    print("‚úÖ POSITIONING: Processed column after 'Website'") 
    print("‚úÖ RESUME: Tracks all processed organizations (not just successful ones)")
    print("‚úÖ FIXED: Directory filtering applied to ALL matches (no glassdoor/linkedin)")
    print("‚úÖ MAINTAINED: All domain matching logic, Memorial special case, etc.")
    print("‚úÖ SKIP: Still skips problematic SHIFT PHYSICAL THERAPY")
    
    # ============= CONFIGURATION =============
    API_KEY = "AIzaSyA7IDHP19xsyec5zt79uMvOgiL6JyNHiZI"
    SEARCH_ENGINE_ID = "43d208b4118ca4ceb"
    
    INPUT_FILE = "npi_multi_service_organizations.csv"
    OUTPUT_FILE = "npi_multi_service_organizations_with_websites_v33.csv"
    
    MAX_API_CALLS = 1450
    # =========================================
    
    if API_KEY == "YOUR_GOOGLE_API_KEY_HERE" or SEARCH_ENGINE_ID == "YOUR_SEARCH_ENGINE_ID_HERE":
        print("\n‚ùå Please add your Google API credentials!")
        print("   Set API_KEY and SEARCH_ENGINE_ID in the configuration section")
    else:
        print(f"\nInput: {INPUT_FILE}")
        print(f"Output: {OUTPUT_FILE}")
        print(f"Max API calls: {MAX_API_CALLS}")
        print("\nStarting in 3 seconds...")
        time.sleep(3)
        
        updated_df = process_organizations_with_resume(
            INPUT_FILE,
            OUTPUT_FILE,
            API_KEY,
            SEARCH_ENGINE_ID,
            MAX_API_CALLS
        )
